tools:
  vforwater_loader:
    title: Dataset Loader
    description: |
      This tool will use `metacatalog` to load datasets stored in a metacatalog instance, like V-FOR-WaTer.
      The requested datasources will be made available in the output directory of the tool. Areal datasets
      will be clipped to the **bounding box** of the reference area and multi-file sources are preselected
      to fall into the time range specified. 
      Note that exact extracts (specific time step, specific area) are not yet supported for areal datasets.
    parameters:
      dataset_ids: 
        type: integer
        array: true
      reference_area:
        type: struct
        description: |
          The reference area can be any valid GeoJSON POLYGON geometry. Datasets that contain areal information will be clipped to this area.
          Be aware, that some remote sensing datasets may have global coverage. If you omit this parameter, the full dataset will be loaded,
          if the hosting server allows it.
          Please make sure, that you only pass one FEATURE. FeatureCollections are not supported yet.
        optional: true
      start_date:
        type: datetime
        description: |
          The start date of the datasetm, if a time dimension applies to the dataset.
      end_date:
        type: datetime
        description: |
          The end date of the datasetm, if a time dimension applies to the dataset.
      integration:
        type: enum
        values:
          - none
          - all
          - spatial
          - temporal
        description: |
          The mode of operation for the integration of each all data files associated to each data source
          into a common DuckDB-based dataset. This dataset includes data for a unified spatial and temporal
          extent and includes macros for aggregation. By setting `integration` the default integrations are
          selected. The resulting database can still be used to query different aggregation levels. 
          - `none`: No integration will be performed and the DuckDB database will **NOT** be created.
          - `all`: Temporal, spatial and spatio-temporal scale aggregations will be integrated, if the scale is defined in the dataset metadata.
          - `spatial`: Only results for spatial aggregations will be provided.
          - `temporal`: Only results for temporal aggregations will be provided.
          - `spatiotemporal`: Only results for spatio-temporal aggregations will be provided.
        optional: true
      keep_data_files:
        type: boolean
        optional: true
        description: |
          If set to `false`, the data files clipped to the spatial and temporal scale as defined in the 
          data-source metadata will not be kept. This saves a lot of disk space for the output.
          If set to `true` (default behavior), then there will be a `/out/datasets` directory in the output.
      precision:
        type: enum
        values:
          - minute
          - hour
          - day
          - month
          - year
          - decade
        optional: true
        description: |
          The precision for aggregations along the temporal scale of the datasets. This parameter does only take effect if
          the integration includes a data integration along the temporal scale. That includes: `temporal`, `spatiotemporal` and `all`.
          If integration is set set and no precision is supplied, the tool chooses a suitable precision. This decision is not
          yet deterministic.
      resolution:
        type: integer
        optional: true
        description: | 
          The resolution of the output data. This parameter is only relevant for areal datasets. If the dataset is not areal, this parameter
          is ignored. If the dataset is areal and the parameter is not set, the original resolution of the dataset is used. 
          If the dataset is areal and the parameter is set, the dataset will be resampled to the given resolution.
          Note: Right now, the aggregations can only write parquet files. For larger (espeically spatio-temporal) datasets, these
          aggreations can be large. A future version will write netCDF or Zarr for these cases.
